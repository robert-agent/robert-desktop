# Robert's Generation Architecture: The Split-Brain Approach

## 1. Partitioned Memory System
Robert employs a strict separation of concerns in its memory architecture, organizing data into distinct graph partitions:
*   **Personal Graph**: Private user data, preferences, and biographical context.
*   **Professional Graph**: Work-related context, projects, and career history.
*   **Business Graph**: Enterprise-level data, organizational knowledge, and shared resources.

Both **original source data** (emails, docs) and **synthetic data** (summaries, insights generated by the system) are ingested and nodes are created within these specific graphs.

## 2. Agentic GraphRAG Loop
Retrieval is not a static lookup but an active, agentic process. Robert utilizes an **Agentic RAG loop** built on **GraphRAG** principles:
*   **Cross-Partition Search**: The agent can traverse relationships across personal, professional, and business graphs to find connections that simple vector search would miss.
*   **Multi-Hop Reasoning**: By following edges in the graph, the system retrieves context that is conceptually related, not just semantically similar.

## 3. The Role of Local Compute (Edge AI)
Robert leverages a small, efficient **Local LLM** to perform critical pre-processing steps before any data leaves the device:
*   **Synthesis & PII Redaction**: The local model synthesizes raw source material and aggressively scrubs Personal Identifiable Information (PII) to ensure privacy.
*   **Prompt Engineering & Compression**: Instead of sending raw chunks to the cloud, the local model structures and compresses the context into highly optimized prompts.
*   **Token Economy**: This pre-processing significantly reduces the token count sent to the cloud, lowering costs and latency.

## 4. Hybrid Framework Integration
The system unifies multiple technologies into a cohesive framework:
*   **Graph Database**: For relational and structural context.
*   **Document Database**: For storing raw, unstructured content.
*   **Agentic RAG**: For intelligent, graph-based context gathering.
*   **Agentic Retrieval**: For tool-based data discovery.

## 5. Core Philosophy: Decoupling Memory from Reasoning
The ultimate goal of this architecture is to split the cognitive load:
*   **Access to Memory (Simple Reasoning)**: Handled locally. The system uses simple reasoning to navigate the graph, find relevant facts, and prepare the context.
*   **Heavy Reasoning (Large Scale Inference)**: Handled by the Cloud (Large LLMs). Once the context is perfectly curated and sanitized, the heavy lifting of planning, complex inference, and creative generation is offloaded to the most powerful models available.

## 6. Market Comparison: Competing RAG Architectures
*   **Naive RAG**: The basic "embed-search-retrieve" loop. Fast and cheap but suffers from low precision and lacks reasoning. It retrieves based on semantic similarity, often missing connected concepts.
*   **Modular RAG**: A step up, allowing swappable components (different retrievers, re-rankers). More flexible but still largely linear.
*   **GraphRAG (Microsoft)**: A research-heavy implementation that builds massive knowledge graphs. Powerful but computationally expensive (10-100x more tokens) and often Python-centric.
*   **Robert's Advantage**: By combining **Agentic Retrieval** (tool use) with **GraphRAG** principles on a **Local** partitioned graph, Robert achieves the depth of GraphRAG with the privacy and speed of edge computing, avoiding the massive cloud costs of Microsoft's approach.

## 7. Open Source Components for Robert
Instead of building from scratch, Robert leverages the high-performance Rust ecosystem:
*   **Graph Database**: **SurrealDB** (embedded, multi-model) or **GraphLite** (lightweight, embedded).
*   **Vector Storage**: **Qdrant** (Rust-based, high performance) or **LanceDB** (embedded, serverless).
*   **RAG Frameworks**: **GraphRAG-rs** (modular Rust implementation) or **Rig** (Rust library for LLM apps).
*   **Local Inference**: **Ollama** (for easy model management) or **Candle** (HuggingFace's pure Rust ML framework) for running small, efficient models like Phi-4 or Gemma 2 locally.

